<div data-ntes="ntes_mail_body_root" style="line-height:1.7;color:#000000;font-size:14px;font-family:Arial"><div id="spnEditorContent"><p style="margin: 0;"><br></p><p style="margin: 0;"><br></p><p style="margin: 0;"><br></p><p style="margin: 0;"><br></p><p style="margin: 0;"><br></p></div><div style="position:relative;zoom:1"></div><div id="divNeteaseMailCard"></div><p style="margin: 0;"><br></p><div id="isForwardContent">-------- Forwarding messages --------<br>From: "Github Action" &lt;lizhi_cursor02@163.com&gt;<br>Date: 2025-03-19 09:18:50<br>To:  You &lt;lz59970062@163.com&gt;<br>Subject: Daily arXiv 2025/03/19<br>

  <style>
    .star-wrapper {
      font-size: 1.3em; 
      line-height: 1; 
      display: inline-flex;
      align-items: center; 
    }
    .half-star {
      display: inline-block;
      width: 0.5em; 
      overflow: hidden;
      white-space: nowrap;
      vertical-align: middle;
    }
    .full-star {
      vertical-align: middle;
    }
  </style>



<div>
    <br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Xin Jin, Haisheng Su, Kai Liu, Cong Ma, Wei Wu, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12009
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> UniMamba proposes a unified architecture integrating 3D convolution and State Space Models to efficiently capture both local and global spatial dependencies from LiDAR point clouds, overcoming challenges of locality loss and limited spatial diversity.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12009v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ruoyu Wang, Yukai Ma, Yi Yao, Sheng Tao, Haoang Li, ...
            <br>
            <i>Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang Guoli Security Technology Co., Ltd., School of Computation, Information and Technology, Technical University of Munich</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12369
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> L2COcc proposes a lightweight camera-centric semantic scene completion framework that incorporates LiDAR inputs, using efficient voxel transformers and cross-modal knowledge distillation modules to significantly reduce computational burden while maintaining high accuracy, outperforming state-of-the-art methods on both SemanticKITTI and SSCBench-KITTI-360 benchmarks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12369v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, ...
            <br>
            <i></i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13111
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces a novel supervised fine-tuning dataset, CA-VQA, for multimodal large language models (MLLMs) to enhance their 3D spatial understanding capabilities, including tasks such as spatial relationship prediction, metric size and distance estimation, and 3D grounding, achieving state-of-the-art performance on 3D spatial understanding benchmarks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13111v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Rui Qian, Chenyangguang Zhang, Yan Di, Guangyao Zhai, Ruida Zhang, ...
            <br>
            <i>Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Chair for Computer Aided Medical Procedures and Augmented Reality, Technical University of Munich</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12086
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces FA-BARF, a novel frequency-adapted bundle-adjusting neural radiance field method that accelerates the joint optimization process of camera poses and 3D scene reconstruction, mitigating frequency fluctuations and improving the robustness of NeRF under imperfect camera poses.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12086v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            SFMNet: Sparse Focal Modulation for 3D Object Detection
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Oren Shrout, Ayellet Tal
            <br>
            <i>Technion</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12093
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces SFMNet, a novel 3D sparse detector that combines the efficiency of sparse convolutions with transformer-like long-range dependencies through a Sparse Focal Modulation module, achieving state-of-the-art performance on autonomous driving datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12093v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            GS-3I: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Tengfei Wang, Yongmao Hou, Zhaoning Zhang, Yiwei Xu, Zongqian Zhan, ...
            <br>
            <i>Wuhan University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12335
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> GS-3I: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images introduces a method that mitigates bias from underexposed regions and geometric constraint mismatches caused by inconsistent lighting, achieving robust and accurate surface reconstruction across various illumination conditions.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12335v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, ...
            <br>
            <i>Beijing Key Laboratory of Big Data Management and Analysis Methods, Dept. of Comp. Sci. \&amp; Tech., BNRist Center, Tsinghua University, ByteDance Seed, Gaoling School of AI, Renmin University of China</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13265
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces FlexWorld, a novel framework that progressively expands 3D scenes from a single image using a fine-tuned video-to-video diffusion model, enabling high-quality novel view synthesis and flexible 3D scene generation with 360° rotations and zooming.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13265v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Atlas: Multi-Scale Attention Improves Long Context Image Modeling
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Kumar Krishna Agrawal, Long Lian, Longchao Liu, Natalia Harguindeguy, Boyi Li, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12355
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> Atlas, a novel neural network architecture leveraging Multi-Scale Attention, significantly improves the compute-performance tradeoff in long-context image modeling, achieving comparable accuracy to state-of-the-art models while being significantly faster.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12355v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Feng Qiao, Zhexiao Xiong, Eric Xing, Nathan Jacobs
            <br>
            <i>Washington University in St. Louis</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12720
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> GenStereo is a novel diffusion-based framework that addresses both visual quality and geometric accuracy in stereo image generation, enabling high-quality stereo images with strong semantic and geometric consistency, and significantly improving unsupervised stereo matching learning across diverse real-world scenarios.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12720v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            TACO: Taming Diffusion for in-the-wild Video Amodal Completion
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ruijie Lu, Yixin Chen, Yu Liu, Jiaxiang Tang, Junfeng Ni, ...
            <br>
            <i>State Key Laboratory of General Artificial Intelligence, BIGAI, Tsinghua University, State Key Laboratory of General Artificial Intelligence, Peking University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12049
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces TACO, a conditional diffusion model for Video Amodal Completion (VAC), which learns rich spatio-temporal consistent manifolds from pre-trained video diffusion models and effectively generalizes to diverse in-the-wild videos, showcasing its potential in various downstream tasks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12049v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Guibiao Liao, Qing Li, Zhenyu Bao, Guoping Qiu, Kanglin Liu
            <br>
            <i>University of Nottingham, Peking University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12535
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces SPC-GS, a novel approach that combines Scene-layout-based Gaussian Initialization and Semantic-Prompt Consistency to address the challenges of sparse input views in 3D Gaussian Splatting-based indoor open-world free-view synthesis, significantly improving reconstruction and semantic segmentation quality.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12535v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            ChangHee Yang, Hyeonseop Song, Seokhun Choi, Seungwoo Lee, Jaechul Kim, ...
            <br>
            <i>AI Lab, CTO Division, LG Electronics</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13025
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> PoseSyn synthesizes diverse 3D pose data from abundant in-the-wild 2D pose datasets by identifying challenging poses, generating motion sequences, and synthesizing realistic images, thereby improving 3D pose estimation accuracy across various real-world scenarios without requiring costly 3D annotations.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13025v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yuheng Jiang, Zhehao Shen, Chengcheng Guo, Yu Hong, Zhuo Su, ...
            <br>
            <i>Max Planck Institute for Informatics, Saarland Informatics Campus, ShanghaiTech University, DGene, ByteDance, NeuDim</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12242
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> RePerformer presents a novel Gaussian-based approach for generating high-fidelity human-centric volumetric videos that can both accurately replay and realistically re-perform dynamic scenes under novel motions, overcoming limitations of existing methods that either focus on general dynamic scenes or animate human avatars.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12242v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, Tat-Jen Cham
            <br>
            <i>University of Oxford, Nanyang Technological University, Singapore Institute of Technology</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13439
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces Amodal3R, a novel single-stage 3D reconstruction model that directly reconstructs complete and high-quality 3D objects from partially occluded 2D images, leveraging mask-weighted cross-attention and occlusion-aware layers to improve both geometry and appearance, outperforming existing two-stage methods and establishing a new benchmark for occlusion-aware 3D reconstruction.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13439v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Mo Zhou, Jianwei Wang, Xuanmeng Zhang, Dylan Campbell, Kai Wang, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12348
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> ProbDiffFlow is an efficient training-free framework that estimates probabilistic optical flow distributions from a single image by synthesizing diverse future frames and leveraging a pre-trained optical flow model, overcoming the limitations of existing single-frame methods that rely on labeled training and produce deterministic predictions.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12348v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            ProtoDepth: Unsupervised Continual Depth Completion with Prototypes
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Patrick Rim, Hyoungseob Park, S. Gangopadhyay, Ziyao Zeng, Younjoon Chung, ...
            <br>
            <i>Yale Vision Lab</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12745
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> ProtoDepth introduces a novel prototype-based approach for unsupervised continual depth completion, enabling models to adapt to new domains without forgetting previously learned information, significantly reducing forgetting compared to baselines across indoor and outdoor sequences.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12745v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency for Few-shot View Synthesis
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ingyun Lee, Jae Won Jang, Seunghyeon Seo, Nojun Kwak
            <br>
            <i>Seoul National University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12947
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> DivCon-NeRF significantly enhances both diversity and consistency in ray augmentation for few-shot view synthesis, effectively reducing floaters and visual distortions compared to existing methods, as demonstrated on multiple datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12947v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Thomas Monninger, Md Zafar Anwar, Stanislaw Antol, Steffen Staab, Sihao Ding
            <br>
            <i>Mercedes-Benz Research \&amp; Development North America, University of Stuttgart, University of Southampton</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13430
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> AugMapNet proposes a novel latent BEV grid augmentation technique that significantly enhances vectorized HD map construction by providing dense spatial supervision to vector map decoders, outperforming existing methods on nuScenes and Argoverse2 datasets with minimal parameter overhead.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13430v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Xuying Zhang, Yupeng Zhou, Kai Wang, Yikai Wang, Zhen Li, ...
            <br>
            <i>Nankai University, ByteDance Inc., Tsinghua University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12929
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> AR-1-to-3 proposes a novel next-view prediction paradigm based on diffusion models that first generates views closer to the input view, using them as contextual information to progressively synthesize farther views, significantly improving consistency and fidelity of 3D assets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12929v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            SparseAlign: A Fully Sparse Framework for Cooperative Object Detection
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yunshuang Yuan, Yan Xia, Daniel Cremers, Monika Sester
            <br>
            <i>Leibniz University Hannover, Technical University of Munich</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12982
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> SparseAlign presents a fully sparse framework for cooperative object detection, addressing center feature missing and isolated convolution field issues, achieving superior performance with significantly reduced communication bandwidth compared to dense frameworks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12982v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            STEP: Simultaneous Tracking and Estimation of Pose for Animals and Humans
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Shashikant Verma, Harish Katti, Soumyaratna Debnath, Yamuna Swamy, Shanmuganathan Raman
            <br>
            <i>National Institutes of Health, Indian Institute of Technology Gandhinagar</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13344
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces STEP, a novel transformer-based framework for simultaneously tracking and estimating pose across diverse animal species and humans, addressing challenges in traditional top-down methods by removing the need for keypoint initialization and integrating tracking capabilities.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13344v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            TransDiff: Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Haoxiao Wang, Kaichen Zhou, Binrui Gu, Zhiyuan Feng, Weijie Wang, ...
            <br>
            <i>School of CS, National Key Laboratory for Multimedia Information Processing, Zhejiang University, Tianjin University of Technology, Peking University, ...</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12779
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> TransDiff is a novel single-view RGB-D-based depth completion framework using Denoising Diffusion Probabilistic Models (DDPM) to accurately grasp transparent objects, overcoming challenges posed by their reflection and refraction properties.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12779v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Less Biased Noise Scale Estimation for Threshold-Robust RANSAC
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Johan Edstedt
            <br>
            <i>Linköping University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13433
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper proposes an improved method for estimating the inlier noise scale, which is crucial for robustly estimating relative pose using RANSAC, by addressing biases in previous approaches and integrating it into a RANSAC framework, thereby enhancing the threshold robustness of RANSAC.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13433v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Rui Wang, Quentin Lohmeyer, Mirko Meboldt, Siyu Tang
            <br>
            <i>ETH Zurich</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13176
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> DeGauss, a self-supervised framework using dynamic-static Gaussian splatting, robustly decomposes and reconstructs dynamic and static elements from real-world, in-the-wild videos, achieving state-of-the-art performance across various scenarios without relying on complex heuristics or extensive supervision.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13176v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            SatDepth: A Novel Dataset for Satellite Image Matching
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Rahul Deshmukh, Avinash Kak
            <br>
            <i>Purdue University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12706
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces "SatDepth," a novel dataset for satellite image matching that provides dense ground-truth correspondences and addresses the imbalance in track-angle differences, enabling the evaluation of image matching networks specifically for satellite imagery and outperforming traditional methods by up to 40% in precision.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12706v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Peizhen Zheng, Longfei Wei, Dongjing Jiang, Jianfei Zhang
            <br>
            <i>ThinkX, Canada, MedicineX, Canada</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12001
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper proposes a novel 3D Gaussian Splatting method for high-fidelity dynamic street scene reconstruction, effectively handling moving objects while preserving static scene details through adaptive transparency optimization and iterative refinement of Gaussian points.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12001v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Deblur Gaussian Splatting SLAM
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Francesco Girlanda, Denys Rozumnyi, Marc Pollefeys, Martin R. Oswald
            <br>
            <i>Microsoft, ETH Zürich, University of Amsterdam</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12572
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces Deblur-SLAM, a robust RGB SLAM pipeline that addresses motion blur by estimating sub-frame trajectories, combining frame-to-frame and frame-to-model approaches, and incorporating online loop closure and global bundle adjustment for high-fidelity reconstructions in motion-blurred settings.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12572v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, ...
            <br>
            <i>Department of Computer Science, Renmin University of China, Department of Data Science \&amp; AI, Monash University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12150
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> Point-Cache is a hierarchical cache model that dynamically captures and stores essential distribution clues from online test samples, enabling robust and generalizable point cloud recognition without relying on training data, thus addressing limitations of prior test-time methods.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12150v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            SAM2 for Image and Video Segmentation: A Comprehensive Survey
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Zhang Jiaxing, Tang Hao
            <br>
            <i>School of Software engineering, Sichuan University, School of Computer Science, Peking University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12781
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper provides a comprehensive survey of SAM2, an improved version of the Segment Anything Model, evaluating its performance in image and video segmentation across various domains, highlighting its strengths and limitations, and proposing future development directions.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12781v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Artem Nikonorov, Georgy Perevozchikov, Andrei Korepanov, Nancy Mehta, Mahmoud Afifi, ...
            <br>
            <i>Artificial Intelligence Research Institute, York University, Moscow Institute of Physics and Technologies, Computer Vision Lab, CAIDAS \&amp; IFI, University of Würzburg, Institute for Information Transmission Problems RAS, ...</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11781
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> cmKAN is a versatile hypernetwork-based Kolmogorov-Arnold Network framework that efficiently and accurately maps colors between different distributions, outperforming existing methods across various color-matching tasks, as demonstrated by extensive evaluations and comparisons.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11781v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, ...
            <br>
            <i>City University of Macau, Department of Computer Science and Engineering, Shanghai Jiaotong University, School of Artificial Intelligence and Computer Science, The Hong Kong University of Science and Technology, ...</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12968
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces OptiPMB, a novel 3D multi-object tracking method that employs an optimized Poisson multi-Bernoulli (PMB) filter within the tracking-by-detection framework, incorporating innovative designs for effective track initialization, occluded object track maintenance, and overall performance enhancement, achieving superior accuracy compared to state-of-the-art methods on nuScenes and KITTI datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12968v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yunze Liu, Peiran Wu, Cheng Liang, Junxiao Shen, Limin Wang, ...
            <br>
            <i>University of Bristol, IIIS, Tsinghua University, Nanjing University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12332
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The key contribution of this paper is the introduction of VideoMAP, a Hybrid Mamba-Transformer framework with a novel frame-wise masked autoregressive pre-training strategy, which significantly improves scalability and sample efficiency in video understanding tasks, outperforming existing methods across various datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12332v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Zihan Zhoua, Changrui Daia, Aibo Songa, Xiaolin Fang
            <br>
            <i>School of Computer Science and Engineering, Southeast University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12026
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces MER, a self-supervised Video Correspondence Learning framework that enhances motion details and pixel correspondence through a Motion Enhancement Engine and Multi-Cluster Sampler, outperforming state-of-the-art methods in video object segmentation and keypoint tracking tasks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12026v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Efficient Multimodal 3D Object Detector via Instance-Level Contrastive Distillation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Zhuoqun Su, Huimin Lu, Shuaifeng Jiao, Junhao Xiao, Yaonan Wang, ...
            <br>
            <i>College of Electrical and Information Engineering, Hunan University, College of Intelligence Science and Technology, National University of Defense Technology</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12914
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces an efficient multimodal 3D object detector that uses Instance-level Contrastive Distillation (ICD) to align image and LiDAR features, along with a Cross Linear Attention Fusion Module (CLFM) for scalable fusion, achieving state-of-the-art performance on the KITTI and nuScenes datasets while maintaining high efficiency.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12914v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Haozhe Si, Yuxuan Wan, Minh Do, Deepak Vasisht, Han Zhao, ...
            <br>
            <i>IBM Research, NY, USA., University of Illinois Urbana-Champaign, IL, USA.</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12843
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces LESS ViT, a novel architecture for geospatial foundation models that incorporates Tied Patch Embedding, Continuous Positional-Channel Embedding, and a Perception Field Mask to efficiently handle spatial-spectral correlations in multi-modal and hyperspectral geospatial data, achieving superior performance with reduced computational complexity and fewer parameters.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12843v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Tianle Li, Yongming Rao, Winston Hu, Yu Cheng
            <br>
            <i>The Chinese University of Hong Kong, Tencent Hunyuan Research</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12446
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> BREEN, a data-efficient encoder-free multimodal architecture, introduces a learnable query to transfer visual knowledge from a pretrained CLIP model, significantly reducing training data requirements while achieving comparable performance to existing encoder-free models.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12446v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Siyuan Yao, Yang Guo, Yanyang Yan, Wenqi Ren, Xiaochun Cao
            <br>
            <i>Beijing University of Posts and Telecommunications, Sun Yat-sen University, State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12888
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> UncTrack is a novel uncertainty-aware transformer tracker that predicts target localization uncertainty and incorporates this information to maintain reliable target state prediction, outperforming state-of-the-art methods across various challenging scenarios.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12888v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            <a href="https://github.com/ManOfStory/UncTrack" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #5bc0de; padding: 8px 16px; border-radius: 4px; margin-left: 8px;">Code</a>
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Dingning Liu, Cheng Wang, Peng Gao, Renrui Zhang, Xinzhu Ma, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13185
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> 3DAxisPrompt enhances the 3D understanding and reasoning capabilities of MLLMs like GPT-4o by leveraging 3D coordinate axes and masks, demonstrating their effectiveness in various 3D tasks through comprehensive evaluations.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13185v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Utkarsh Nath, Rajeev Goel, Rahul Khurana, Kyle Min, Mark Ollila, ...
            <br>
            <i>Arizona State University, Stability AI, Intel Labs</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11981
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> DecompDreamer is a novel text-to-3D generation framework that effectively handles complex prompts with multiple objects and intricate inter-object relationships through structured decomposition and progressive optimization, ensuring high-quality and coherent 3D compositions with enhanced object disentanglement.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11981v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF Pose Estimation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Nassim Ali Ousalah, Anis Kacem, Enjie Ghorbel, Emmanuel Koumandakis, Djamila Aouada
            <br>
            <i>ENSI, University of Luxembourg, Infinite Orbits</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13053
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces an uncertainty-aware Knowledge Distillation framework for compact and efficient 6DoF pose estimation, leveraging the varying levels of uncertainty in teacher model predictions to improve student model accuracy while maintaining compactness.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13053v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Jiaxu Liu, Li Li, Hubert P. H. Shum, Toby P. Breckon
            <br>
            <i>Durham University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13004
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper proposes a novel point cloud diffusion framework, TFDM, which integrates frequency analysis and the Mamba architecture to efficiently generate high-quality 3D point clouds, achieving state-of-the-art performance while significantly reducing computational resources and inference time.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13004v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Zhe Shan, Yang Liu, Lei Zhou, Cheng Yan, Heng Wang, ...
            <br>
            <i>Hainan University, Tianjin University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12006
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> ROS-SAM, a refined version of the Segment Anything Model (SAM), addresses the challenges of remote sensing moving object segmentation by leveraging LoRA fine-tuning, enhanced feature discriminability, and integration of global context with local details, significantly improving segmentation quality and generalization across diverse remote sensing datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12006v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering, Tracking, Motion Predictions of Moving Objects in Dynamic Scenes
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Runfa Blark Li, Mahdi Shaghaghi, Keito Suzuki, Xinshuang Liu, Varun Moparthi, ...
            <br>
            <i>UC San Diego, Qualcomm XR Advanced Technology</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11979
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> DynaGSLAM is a real-time Gaussian-Splatting SLAM algorithm that jointly estimates accurate ego motion while online rendering, tracking, and predicting the motion of moving objects in dynamic scenes, overcoming the limitations of previous GS-SLAM approaches by managing dynamic regions explicitly and efficiently.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11979v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Katja Schwarz, Norman Mueller, Peter Kontschieder
            <br>
            <i>Meta Reality Labs Zurich</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13272
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> Generative Gaussian Splatting (GGS) integrates a 3D representation with pre-trained latent video diffusion models to generate 3D scenes with improved 3D consistency and quality, significantly outperforming existing baselines on RealEstate10K and ScanNet+ datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13272v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            GIFT: Generated Indoor video frames for Texture-less point tracking
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Jianzheng Huang, Xianyu Mo, Ziling Liu, Jinyu Yang, Feng Zheng
            <br>
            <i>Southern University of Science and Technology, tapall.ai</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12944
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces GIFT, a synthetic indoor video dataset designed to evaluate point tracking methods in texture-less or weakly textured regions, using metrics to classify 3D models into different texture intensity levels and providing detailed annotations for each video sequence.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12944v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            AnyCalib: On-Manifold Learning for Model-Agnostic Single-View Camera Calibration
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Javier Tirado-Garín, Javier Civera
            <br>
            <i>University of Zaragoza</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12701
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> AnyCalib is a novel, model-agnostic single-view camera calibration method that uses FoV fields, a robust intermediate representation not tied to extrinsic cues, to accurately recover intrinsics for a wide range of camera models, including edited and distorted images, outperforming existing methods across various benchmarks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12701v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Henghui Du, Guangyao Li, Chang Zhou, Chunjie Zhang, Alan Zhao, ...
            <br>
            <i>Gaoling School of Artificial Intelligence, Renmin University of China, AI Technology Center, Online Video Business Unit, Tencent PCG</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13068
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper proposes a unified audio-visual scene understanding model with explicit cooperation, addressing the heterogeneity and complex relationships among tasks by refining datasets and designing interaction-aware LoRA structures to facilitate concrete cooperation in learning, outperforming existing models on multiple tasks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13068v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Md Abu Bakr Siddique, Junliang Liu, Piyush Singh, Md Jahidul Islam
            <br>
            <i>University of Florida</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11893
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> UStyle is a novel data-driven framework for underwater waterbody style transfer that preserves structural integrity while achieving perceptually consistent stylization through a depth-aware whitening and coloring transform, outperforming existing methods in underwater image style transfer.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11893v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Tao Wang, Changxu Cheng, Lingfeng Wang, Senda Chen, Wuyue Zhao
            <br>
            <i>Zhejiang University, Uni-Ubi, Tongji University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13026
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> HiMTok proposes an efficient hierarchical mask tokenizer that represents segmentation masks using up to 32 hierarchical tokens, enabling large multimodal models to learn image segmentation capabilities without requiring an image-conditioned mask decoder or off-the-shelf segmentation models, while also improving visual grounding and maintaining general image understanding.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13026v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            MTGS: Multi-Traversal Gaussian Splatting
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Tianyu Li, Yihang Qiu, Zhenhua Wu, Carl Lindström, Peng Su, ...
            <br>
            <i>The University of Hong Kong, Shanghai Innovation Institute, Technical University of Munich, Chalmers University of Technology</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12552
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces MTGS, a novel approach for reconstructing high-fidelity driving scenes from multi-traversal data, which models shared static geometry while separately handling dynamic elements and appearance variations, significantly improving view extrapolation quality compared to single-traversal baselines.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12552v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Hao Yin, Guangzong Si, Zilei Wang
            <br>
            <i>University of Science and Technology of China</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13108
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper uncovers the phased processing of visual information in multimodal large language models, proposing that image tokens primarily interact with instruction tokens in shallow layers to inject most visual information, and consolidate the remaining visual information in deeper layers, leading to improved inference efficiency through Hierarchical Modality-Aware Pruning (HiMAP).
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13108v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            <a href="https://github.com/ustc-hyin/HiMAP" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #5bc0de; padding: 8px 16px; border-radius: 4px; margin-left: 8px;">Code</a>
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yi Zhang, Wenye Zhou, Ruonan Lin
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13179
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This study introduces a lightweight super-resolution method for building images using a Dilated Contextual Feature Modulation Network (DCFMN) that efficiently models long-range dependencies and enhances local features, outperforming existing lightweight networks in accuracy and efficiency.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13179v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Byeongjun Park, Hyojun Go, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, ...
            <br>
            <i>EverEx, KAIST, Yonsei University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12024
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> SteerX is a zero-shot inference-time steering method that unifies camera-free 3D and 4D scene generation and reconstruction, iteratively tilting data distributions toward geometrically consistent samples using tailored geometric reward functions and sequential Monte Carlo sampling, effectively improving alignment across various scene generation tasks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12024v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            <a href="https://github.com/byeongjun-park/SteerX" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #5bc0de; padding: 8px 16px; border-radius: 4px; margin-left: 8px;">Code</a>
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Songen Gu, Haoxuan Song, Binjie Liu, Qian Yu, Sanyi Zhang, ...
            <br>
            <i>Institute of Software, CAS, Beihang University, Communication University of China</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12383
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> VRsketch2Gaussian introduces a novel framework for VR sketch-guided 3D object generation using 3D Gaussian Splatting, leveraging a large-scale paired VR sketch dataset (VRSS) and innovative alignment techniques to bridge the gap between sparse sketches and rich text descriptions, enabling high-fidelity 3D native generation with fine geometric and visual control.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12383v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Beyond RGB: Adaptive Parallel Processing for RAW Object Detection
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Shani Gamrian, Hila Barel, Feiran Li, Masakazu Yoshimura, Daisuke Iso
            <br>
            <i>Institution A, Sony AI</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13163
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces RAM, a novel Raw Adaptation Module designed to optimize object detection on RAW images by replacing traditional sequential ISP pipelines with parallel processing, thereby preserving and leveraging the full sensor data for superior performance across various RAW datasets and challenging conditions.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13163v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Point Cloud Based Scene Segmentation: A Survey
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Dan Halperin, Niklas Eisl
            <br>
            <i></i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12595
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This survey provides an in-depth overview of the current state-of-the-art methods in Point Cloud Semantic Segmentation for autonomous driving, categorizing them into projection-based, 3D-based, and hybrid approaches, and comparing their segmentation accuracy and efficiency.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12595v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Anthony Lamelas, Harrison Muchnic
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12617
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The study finds that while scaling semantic categories initially improves vision transformer labeling accuracy, beyond a certain point, the benefits diminish or reverse, providing insights into the limitations of category labeling strategies for ViTs.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12617v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Nhi Pham, Bernt Schiele, Adam Kortylewski, Jonas Fischer
            <br>
            <i>Max Planck Institute for Informatics, Saarland Informatics Campus, University of Freiburg</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13429
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces CAVE, a robust and inherently interpretable image classifier that learns concept-aware neural object volumes, thereby addressing the limitations of existing 3D-aware classifiers by providing spatially consistent and meaningful concepts that enhance both interpretability and robustness against out-of-distribution data.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13429v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Jiaming Kang, Keyan Chen, Zhengxia Zou, Zhenwei Shi
            <br>
            <i>Beihang University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13347
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> TriDF, a hybrid 3D representation, efficiently accelerates remote sensing novel view synthesis from as few as 3 input views, achieving superior rendering quality and significantly faster reconstruction speed compared to NeRF-based methods.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13347v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ruyu Wang, Xuefeng Hou, Sabrina Schmedding, Marco F. Huber
            <br>
            <i>Bosch Center for Artificial Intelligence, Renningen, Germany, Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Stuttgart, Germany, Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12213
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces STAY Diffusion, a diffusion-based model that generates photo-realistic images from layout inputs, providing fine-grained control over stylized objects in scenes through novel normalization and attention mechanisms, outperforming previous state-of-the-art methods in generation diversity, accuracy, and controllability.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12213v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ans Munir, Faisal Z. Qureshi, Muhammad Haris Khan, Mohsen Ali
            <br>
            <i>Information Technology University, University of Ontario Institute of Technology, Mohamed Bin Zayed University of AI</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12206
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces TLAC, a training-free approach that leverages Large Multimodal Models (LMMs) like Gemini to achieve superior image classification accuracy on multiple datasets without requiring any fine-tuning, significantly improving adaptability and computational efficiency.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12206v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yinqiao Wang, Hao Xu, Pheng-Ann Heng, Chi-Wing Fu
            <br>
            <i>Department of Computer Science and Engineering, Institute of Medical Intelligence and XR, The Chinese University of Hong Kong</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13303
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> UniHOPE, the proposed unified approach, addresses the limitations of existing methods by flexibly handling both hand-only and hand-object scenarios through an end-to-end design that dynamically switches between object pose estimation and adaptively fuses effective object features, while also learning robust occlusion-invariant hand features from generated de-occluded images.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13303v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Jiahao Wu, Rui Peng, Zhiyan Wang, Lu Xiao, Luyang Tang, ...
            <br>
            <i>Peking University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12307
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> Swift4D innovatively decomposes dynamic and static primitives using 2D multi-view images, employs a compact multi-resolution 4D Hash for efficient temporal modeling of dynamic points, and combines both to achieve state-of-the-art rendering quality with significantly reduced training time and storage requirements.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12307v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, ...
            <br>
            <i>HKUST, CUHK</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12496
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces LSDBench, a benchmark to evaluate Large Vision-Language Models on long-video tasks by focusing on Necessary Sampling Density, and proposes a Reasoning-Driven Hierarchical Sampling framework along with a Semantic-Guided Frame Selector to address the sampling dilemma in long videos.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12496v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Feihong Yan, Qingyan Wei, Jiayi Tang, Jiajun Li, Yulin Wang, ...
            <br>
            <i>University of Electronic Science and Technology of China, Shanghai Jiaotong University, Hong Kong University of Science and Technology(Guangzhou), Central South University, China University of Mining and Technology, ...</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12450
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> LazyMAR accelerates Masked Autoregressive models by introducing token and condition caches to handle redundancies in adjacent decoding steps, achieving up to 2.83 times acceleration with minimal quality degradation, and being plug-and-play for all MAR models.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12450v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yiwei Xu, Yifei Yu, Wentian Gan, Tengfei Wang, Zongqian Zhan, ...
            <br>
            <i>University of Twente, Wuhan University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13086
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper presents On-the-Fly Gaussian Splatting (3DGS), a progressive framework that enables near real-time optimization of 3DGS during image capture, significantly reducing training time and maintaining high rendering quality.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13086v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            An interpretable approach to automating the assessment of biofouling in video footage
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Evelyn J. Mannix, Bartholomew A. Woodham
            <br>
            <i>The University of Melbourne, Department of Agriculture, Fisheries and Forestry</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12875
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper presents an interpretable approach using Component Features (ComFe) with a DINOv2 Vision Transformer (ViT) foundation model to automate the assessment of biofouling in video footage of vessel hulls, outperforming previous methods while providing transparency and explainability in predictions.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12875v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Wenbo Dai, Lijing Lu, Zhihang Li
            <br>
            <i>Chinese Academy of Sciences, Peking University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12472
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces DiVE, a novel method for automatically generating large-scale RGB-IR paired images with consistent identities, significantly enhancing the performance of VI-ReID models by decoupling identity and modality, thus addressing the challenges of data scarcity and privacy concerns in VI-ReID tasks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12472v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Sung-Yeon Park, Can Cui, Yunsheng Ma, Ahmadreza Moradipari, Rohit Gupta, ...
            <br>
            <i>Purdue University, Toyota InfoTech Labs</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12772
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> NuPlanQA-Eval and NuPlanQA-1M provide a comprehensive evaluation benchmark and large-scale dataset for multi-modal large language models in driving scene understanding, highlighting the challenges and potential of these models in complex traffic scenarios.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12772v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ling Yang, Kaixin Zhu, Juanxi Tian, Bohan Zeng, Mingbao Lin, ...
            <br>
            <i>National University of Singapore, Peking University, University of Chinese Academy of Sciences</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13435
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces a novel 4D reconstruction benchmark, WideRange4D, which includes rich 4D scene data with wide-range spatial movements, enhancing the evaluation of 4D generation methods and proposing Progress4D, a new method that achieves stable and high-quality 4D results across various complex 4D scene reconstruction tasks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13435v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Bangzheng Li, Fei Wang, Wenxuan Zhou, Nan Xu, Ben Zhou, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11794
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> Semantic-Clipping: A novel framework that integrates textual semantics into the visual encoding process of Vision-Language Models (VLMs), significantly enhancing their ability to process fine-grained visual details without requiring additional training, thereby improving VQA performance and reducing computational overhead.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11794v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            LangDA: Building Context-Awareness via Language for Domain Adaptive Semantic Segmentation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Chang Liu, Bavesh Balaji, Saad Hossain, C Thomas, Kwei-Herng Lai, ...
            <br>
            <i>Apple, University of Waterloo</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12780
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> LangDA addresses the challenges of unsupervised domain adaptation for semantic segmentation by learning context-aware representations through language, explicitly capturing spatial relationships between objects in images and achieving state-of-the-art performance across multiple benchmarks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12780v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ying Zang, Yuncan Gao, Jiangi Zhang, Yuangi Hu, Runlong Cao, ...
            <br>
            <i>University of Science and Technology of China, KOKONI 3D, Moxin (Huzhou) Tech. Co., LTD., Zhejiang University, Huzhou University, Singapore University of Technology and Design</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12191
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This work introduces a novel sketch-based prompting method and the first dataset pairing human sketches with remote sensing imagery, enhancing zero-shot interactive segmentation for remote sensing images and significantly improving accuracy and robustness compared to state-of-the-art methods.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12191v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Weiguang Zhao, Rui Zhang, Qiufeng Wang, Guangliang Cheng, Kaizhu Huang
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12539
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces BFANet, a novel 3D semantic segmentation framework that categorizes and evaluates four types of segmentation errors, incorporating detailed boundary feature analysis to enhance overall performance and outperform state-of-the-art methods on benchmark datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12539v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Tianyi Zhao, Boyang Liu, Yanglei Gao, Yiming Sun, Maoxun Yuan, ...
            <br>
            <i>Southeast University, Beihang University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11780
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper rethinks multi-modal object detection by introducing linear probing evaluation and constructing a novel framework M$^2$D-LIF, which facilitates sufficient mono-modality learning during multi-modal joint training and explores a lightweight yet effective feature fusion method to mitigate the Fusion Degradation phenomenon and improve object detection performance.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11780v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            MOS: Modeling Object-Scene Associations in Generalized Category Discovery
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Zhengyuan Peng, Jinpeng Ma, Zhimin Sun, Ran Yi, Haichuan Song, ...
            <br>
            <i>Shanghai Jiao Tong University, Chongqing University, East China Normal University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12035
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper argues that scene information should be viewed as a valuable prior for inferring novel categories in Generalized Category Discovery, proposing the Modeling Object-Scene Associations (MOS) framework to enhance GCD performance by leveraging scene-awareness modules, achieving significant accuracy improvements on fine-grained datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12035v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Linzhou Li, Yumeng Li, Yanlin Weng, Youyi Zheng, Kun Zhou
            <br>
            <i>State Key Lab of CAD&amp;CG, Zhejiang University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12886
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces RGBAvatar, a method for real-time photorealistic animated head avatar reconstruction from monocular video streams using reduced Gaussian blendshapes, which are learned adaptively from tracked FLAME parameters and optimized for efficiency and quality, significantly reducing the number of blendshapes needed compared to traditional approaches.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12886v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Ruiqi Song, Xianda Guo, Hangbin Wu, Qinggong Wei, Long Chen
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13047
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces InsightDrive, a novel end-to-end autonomous driving method that organizes perception by language-guided scene representation, focusing on regions directly impacting the ego vehicle's movement, and achieves state-of-the-art performance in vision-based planning tasks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13047v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Luming Wang, Hao Shi, Xiaoting Yin, Kailun Yang, Kaiwei Wang
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12419
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces a novel approach for egocentric gesture recognition using event cameras, proposing a dataset and a lightweight neural architecture that achieves high accuracy while maintaining computational efficiency.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12419v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Learning Dual-Domain Multi-Scale Representations for Single Image Deraining
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Shun Zou, Yi Zou, Mingya Zhang, Shipeng Luo, Guangwei Gao, ...
            <br>
            <i>Xiangtan University, Northeast Forestry University, Nanjing Agricultural University, Nanjing University of Posts and Telecommunications, Westlake University, ...</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12014
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces DMSR, a novel dual-domain multi-scale architecture for single image deraining that addresses the limitations of existing methods by leveraging both external and internal multi-scale information, enhancing feature representations and achieving state-of-the-art performance across multiple benchmark datasets.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12014v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Christopher Xie, Armen Avetisyan, Henry Howard-Jenkins, Yawar Siddiqui, Julian Straub, ...
            <br>
            <i></i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11806
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces a human-in-the-loop method for refining 3D scene layouts using local corrections, integrating a novel "infilling" task into a multi-task learning framework to improve the model's ability to correct specific errors, and enabling users to iteratively refine scene estimates via a simple workflow.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11806v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            SPOC: Spatially-Progressing Object State Change Segmentation in Video
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Priyanka Mandikal, Tushar Nagarajan, Alex Stoken, Zihui Xue, Kristen Grauman
            <br>
            <i>The University of Texas at Austin</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11953
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces a novel task of spatially-progressing object state change segmentation in video, proposing a model that segments object regions into actionable and transformed parts and validating its effectiveness on a new benchmark dataset derived from human activity videos.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11953v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yihao Wang, Raphael Memmesheimer, Sven Behnke
            <br>
            <i>University of Bonn</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12230
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> LIAM is an end-to-end multimodal Transformer model that predicts action sequences based on language, image, action, and semantic map inputs, pre-aligning embedding spaces from different modalities to enhance understanding and generalizability, as demonstrated on the ALFRED dataset.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12230v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Segment Any-Quality Images with Generative Latent Space Enhancement
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Guangqian Guo, Yoong Guo, Xuehui Yu, Wenbo Li, Yaoxing Wang, ...
            <br>
            <i>Tencent, Northwestern Polytechnical University, Huawei Noah's Ark Lab, Huawei</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12507
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> GleSAM enhances Segment Anything Models by incorporating generative latent space enhancement, significantly improving robustness on low-quality images with varying degradations while maintaining generalization to clear images, as demonstrated by extensive experiments.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12507v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Clustering is back: Reaching state-of-the-art LiDAR instance segmentation without training
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, Vincent Lepetit
            <br>
            <i>Valeo.ai, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13203
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper demonstrates that competitive LiDAR panoptic segmentation can be achieved using only semantic labels and an unsupervised clustering algorithm, achieving performance comparable to state-of-the-art supervised methods without requiring instance labels, training, or heavy computational resources.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13203v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing Photorealistic Human Avatars
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yifan Zhan, Wangze Xu, Qingtian Zhu, Muyao Niu, Mingze Ma, ...
            <br>
            <i>The University of Tokyo, Shanghai Artificial Intelligence Laboratory</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12751
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> R3-Avatar innovatively combines a temporal codebook with a record-retrieve-reconstruct strategy to generate high-fidelity, animatable 3D human avatars from video data, overcoming limitations in existing methods by ensuring both rendering quality and animation flexibility.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12751v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Scale Efficient Training for Large Datasets
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Qing Zhou, Junyu Gao, Qi Wang
            <br>
            <i>Northwestern Polytechnical University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13385
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces Scale Efficient Training (SeTa), a dynamic sample pruning approach that reduces training time for large datasets by up to 50%, effectively handling redundant, challenging, and inefficient samples, and demonstrating its versatility across various synthetic and real-world datasets and architectures.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13385v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Efficient Motion-Aware Video MLLM
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Zijia Zhao, Yuqi Huo, Tongtian Yue, Longteng Guo, Haoyu Lu, ...
            <br>
            <i>Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13016
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces EMA, an efficient motion-aware video MLLM that utilizes compressed video structures as inputs, integrating dense RGB frames with sparse motion vectors to reduce redundancy and enhance motion representation, achieving state-of-the-art performance on video benchmarks while reducing inference costs.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13016v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            UniReg: Foundation Model for Controllable Medical Image Registration
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Zi Li, Jianpeng Zhang, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, ...
            <br>
            <i>DAMO Academy, Alibaba Group, College of Computer Science and Technology, Zhejiang University, The First Affiliated Hospital of College of Medicine, Zhejiang University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12868
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> UniReg is a novel foundation model for medical image registration that combines the precision of task-specific learning methods with the generalization of traditional optimization methods, enabling a unified framework for diverse registration scenarios with significant computational efficiency improvements.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12868v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Decouple to Reconstruct: High Quality UHD Restoration via Active Feature Disentanglement and Reversible Fusion
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yidi Liu, Dong Li, Yuxin Ma, Jie Huang, Wenlong Zhang, ...
            <br>
            <i>Shanghai AI Laboratory, University of Science and Technology of China</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12764
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper proposes a novel framework, D²R-UHDNet, for ultra-high-definition image restoration that decouples background-dominated and degradation-dominated features through active feature disentanglement and reversible fusion, effectively mitigating information loss and improving computational efficiency while achieving state-of-the-art results.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12764v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Xianming Zeng, Sicong Du, Qifeng Chen, Lizhe Liu, Haoyu Shu, ...
            <br>
            <i>Unmanned Vehicle Department of CaiNiao Inc., Alibaba Group</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11731
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This paper introduces a Gaussian Splatting-based sensor simulation framework that addresses scalability and efficiency challenges in industrial workflows, providing a modular and practical solution for realistic scene modeling, data augmentation, and end-to-end validation of autonomous driving systems.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11731v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            TopoGaussian: Inferring Internal Topology Structures from Visual Clues
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Xiaoyu Xiong, Changyu Hu, Chunru Lin, Pingchuan Ma, Chuang Gan, ...
            <br>
            <i>University of Massachusetts Amherst, Shanghai Qi Zhi Institute, Massachusetts Institute of Technology, Tsinghua University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12343
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> TopoGaussian is a novel, non-intrusive pipeline that infers the internal topology structure of opaque objects from easily accessible photos and videos, overcoming the limitations of traditional mesh-based methods by providing a flexible and efficient particle-based framework that outputs smoother, 3D-print friendly results while being significantly faster and more accurate.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12343v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Chaolong Yang, Kai Yao, Yuyao Yan, Chenru Jiang, Weiguang Zhao, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12963
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> KDTalker combines unsupervised implicit 3D keypoints with a spatiotemporal diffusion model to generate high-quality, diverse, and efficient audio-driven talking portraits with accurate lip synchronization and natural head poses.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12963v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, ...
            <br>
            <i>Northwestern Polytechnical University, Tsinghua University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12797
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces DeepPerception, an MLLM enhanced with cognitive visual perception capabilities, to address the challenges of knowledge-intensive visual grounding (KVG) tasks, achieving significant performance improvements and superior cross-domain generalization compared to direct fine-tuning methods.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12797v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Han Mei, Kunqian Li, Shuaixin Liu, Chengzhi Ma, Qianli Jiang
            <br>
            <i>Ocean University of China</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12470
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces DPF-Net, a two-stage underwater image enhancement network that combines physical imaging models with data-driven methods, integrating physical parameters from synthetic datasets to improve the accuracy and reliability of underwater image restoration, outperforming existing benchmarks.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12470v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D Segmentation
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Yanlin Xiang, Qingyuan He, Ting Xu, Ran Hao, Jiacheng Hu, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12853
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> This study proposes an adaptive Transformer attention and multi-scale fusion method for spine 3D segmentation, significantly improving accuracy and robustness compared to existing methods, as demonstrated by enhanced mIoU, mDice, and mAcc metrics.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12853v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Seunggwan Lee, Hwanhee Jung, Byoungsoo Koh, Qixing Huang, Sangho Yoon, ...
            <br>
            <i>The University of Texas at Austin, Korea University, KOCCA, KAIST</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12834
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> PASTA, a novel approach for sketch-to-3D shape generation, integrates visual and text conditions from a vision-language model to compensate for missing visual cues, enhancing the comprehension of object components in user sketches, and achieves state-of-the-art performance in qualitative and quantitative evaluations through the use of ISG-Net and SPAGHETTI shape decoder.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12834v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale Portrait Few-Step Synthesis
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Shitong Shao, Hongwei Yi, Hanzhong Guo, Tian Ye, Daquan Zhou, ...
            <br>
            <i>Unknown Affiliation</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.13319
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper proposes MagicDistillation (W2SVD), a novel weak-to-strong video distillation method to mitigate memory constraints and training collapse in large-scale video diffusion models, significantly improving the efficiency and quality of portrait video synthesis.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.13319v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Shun Zou, Yi Zou, Mingya Zhang, Shipeng Luo, Zhihao Chen, ...
            <br>
            <i>Xiangtan University, Northeast Forestry University, Nanjing Agricultural University, Nanjing University of Posts and Telecommunications, Beijing Information Science and Technology University, ...</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.11995
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> Fraesormer is an adaptive efficient sparse Transformer architecture that addresses lightweight food recognition challenges by adaptively retaining critical attention values, reducing redundancy, and promoting multi-scale feature exploration, outperforming state-of-the-art methods in food image recognition.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.11995v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>
<br>
    <table border="0" cellpadding="0" cellspacing="0" width="100%" style="font-family: Arial, sans-serif; border: 1px solid #ddd; border-radius: 8px; padding: 16px; background-color: #f9f9f9;">
    <tbody><tr>
        <td style="font-size: 20px; font-weight: bold; color: #333;">
            From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #666; padding: 8px 0;">
            Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng
            <br>
            <i>Stony Brook University, The Chinese University of Hong Kong, Shanghai Artificial Intelligence Laboratory, Fudan University</i>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>Relevance:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>arXiv ID:</strong> 2503.12821
        </td>
    </tr>
    <tr>
        <td style="font-size: 14px; color: #333; padding: 8px 0;">
            <strong>TLDR:</strong> The paper introduces an Adaptive Data Refinement Framework (ADR) to address the long-tail problem in large Vision-Language Models (LVLMs), balancing redundant head data and synthesizing scarce tail data, thereby improving model performance across various benchmarks without increasing training data volume.
        </td>
    </tr>

    <tr>
        <td style="padding: 8px 0;">
            <a href="http://arxiv.org/pdf/2503.12821v1" style="display: inline-block; text-decoration: none; font-size: 14px; font-weight: bold; color: #fff; background-color: #d9534f; padding: 8px 16px; border-radius: 4px;">PDF</a>
            
        </td>
    </tr>
</tbody></table>

</div>

<br><br>
<div>
To unsubscribe, remove your email in your Github Action setting.
</div>



</div></div><img style="width:1px;height:1px" src="https://count.mail.163.com/beacon/webmail.gif?type=webmail_mailtrace&amp;guid=pre_4f27dbb7f29bc2204f4d84184d8e52ba">